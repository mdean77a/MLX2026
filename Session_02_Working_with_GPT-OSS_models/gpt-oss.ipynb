{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e131aae7",
   "metadata": {},
   "source": [
    "## GPT-OSS Models\n",
    "\n",
    "The OpenAI GPT-OSS models do not work using the standard MLX prompting.  \n",
    "\n",
    "To illustrate this, we will use a standard prompt using a Qwen model and then try with a GPT-OSS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729655c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mlx-community/Qwen3-4B-Instruct-2507-4bit\n",
      "(First time may take a while...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f8223009974b63b46514697c1e34c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully!\n",
      "==========\n",
      "The capital of the United States is Washington, D.C. (Washington, District of Columbia). It is located in the eastern part of the country and serves as the seat of the federal government, housing the U.S. Congress, the White House, and other key government institutions. \n",
      "\n",
      "Note: While Washington, D.C. is the capital, it is not one of the 50 U.S. states, but rather a federal district established in 1790. The city is named after George Washington, the first U.S. president. \n",
      "\n",
      "âœ… Correct answer: **Washington, D.C.**.\n",
      "==========\n",
      "Prompt: 9 tokens, 46.225 tokens-per-sec\n",
      "Generation: 125 tokens, 83.959 tokens-per-sec\n",
      "Peak memory: 2.321 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "from utilities import get_model\n",
    "\n",
    "model, tokenizer, _ = get_model(\"qwen\")\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902af59d",
   "metadata": {},
   "source": [
    "There is a fair amount of meandering in this response, and that is because we did not apply any kind of chat template.  Let's do that now with the standard MLX chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6d3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of the United States?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "==========\n",
      "The capital of the United States is Washington, D.C.\n",
      "==========\n",
      "Prompt: 17 tokens, 118.853 tokens-per-sec\n",
      "Generation: 13 tokens, 90.936 tokens-per-sec\n",
      "Peak memory: 2.338 GB\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":user_query}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "else:\n",
    "    prompt = user_query\n",
    "\n",
    "print(prompt)\n",
    "response = generate(model, tokenizer, prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44115288",
   "metadata": {},
   "source": [
    "Clearly that is much better.  You can see that the chat template is applied and the response is much more focused.\n",
    "\n",
    "Now let's try the same with a GPT-OSS model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6088d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mlx-community/gpt-oss-20b-MXFP4-Q4\n",
      "(First time may take a while...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682cd08b4d3d4f0594a513fc39f95a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully!\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2026-01-06\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>What is the capital of the United States?<|end|><|start|>assistant\n",
      "==========\n",
      "<|channel|>analysis<|message|>The user asks: \"What is the capital of the United States?\" They likely mean \"United States\" but likely \"United States\" is a typo for \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" meaning \"United\n",
      "==========\n",
      "Prompt: 76 tokens, 29.368 tokens-per-sec\n",
      "Generation: 256 tokens, 64.929 tokens-per-sec\n",
      "Peak memory: 13.446 GB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, _ = get_model(\"gpt\")\n",
    "\n",
    "user_query = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":user_query}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "else:\n",
    "    prompt = user_query\n",
    "\n",
    "print(prompt)\n",
    "response = generate(model, tokenizer, prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e2536",
   "metadata": {},
   "source": [
    "We never get an answer.  The output looks confusing.  There is a system message, a list of \"valid channels\", and then all of the response is in the \"analysis\" channel.  We're not in Kansas anymore, Toto.  Maybe we need to let it generate more tokens and think longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3907415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = generate(model, tokenizer, prompt, verbose=True, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023283f",
   "metadata": {},
   "source": [
    "That didn't solve our problem but it confirms that the model is in an infinite loop with output to the analysis channel, which represents the \"thinking\".  Remember that these models are reasoning (thinking) models.\n",
    "\n",
    "The solution to our problem is to use the openai-harmony library instead of the default MLX library.  Let's see how that works. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02012d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_harmony import (\n",
    "    HarmonyEncodingName,\n",
    "    Message,\n",
    "    load_harmony_encoding,\n",
    "    Conversation,\n",
    "    Message,\n",
    "    Role,\n",
    "    SystemContent,\n",
    "    DeveloperContent,\n",
    ")\n",
    "\n",
    "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad0f8c",
   "metadata": {},
   "source": [
    "You have probably had OPENAI API experience with a system prompt as well as the user input. The GPT-OSS models have a different structure, but to make adoption easier, what we have considered a system prompt becomes a \"developer\" message.  The user input is still a \"user\" message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc01a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(author=Author(role=<Role.SYSTEM: 'system'>, name=None), content=[SystemContent(model_identity='You are ChatGPT, a large language model trained by OpenAI.', reasoning_effort=<ReasoningEffort.MEDIUM: 'Medium'>, conversation_start_date=None, knowledge_cutoff='2024-06', channel_config=ChannelConfig(valid_channels=['analysis', 'commentary', 'final'], channel_required=True), tools=None)], channel=None, recipient=None, content_type=None)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "messages = [Message.from_role_and_content(Role.SYSTEM, SystemContent.new())]\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39fc1b",
   "metadata": {},
   "source": [
    "We have some useful utilities to help us understand the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60352bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.harmony_tools import (\n",
    "    print_harmony_messages,\n",
    "    display_harmony_response,\n",
    "    extract_channel_content,\n",
    "    get_final_response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66406a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”µ SYSTEM\n",
      "   Identity: You are ChatGPT, a large language model trained by OpenAI.\n",
      "   Reasoning: ReasoningEffort.MEDIUM\n",
      "   Knowledge Cutoff: 2024-06\n",
      "   Valid Channels: ['analysis', 'commentary', 'final']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_harmony_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f8970d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”µ SYSTEM\n",
      "   Identity: You are ChatGPT, a large language model trained by OpenAI.\n",
      "   Reasoning: ReasoningEffort.MEDIUM\n",
      "   Knowledge Cutoff: 2024-06\n",
      "   Valid Channels: ['analysis', 'commentary', 'final']\n",
      "\n",
      "\n",
      "ğŸ”µ DEVELOPER\n",
      "   Instructions: You are a friendly assistant and you always answer like a Pirate.\n",
      "\n",
      "\n",
      "ğŸ”µ USER\n",
      "   Text: What is the capital of the United States?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a friendly assistant and you always answer like a Pirate.\"\n",
    "user_message = \"What is the capital of the United States?\"\n",
    "\n",
    "if system_message:\n",
    "    messages.append(\n",
    "        Message.from_role_and_content(\n",
    "            Role.DEVELOPER,\n",
    "            DeveloperContent.new().with_instructions(system_message)\n",
    "        )\n",
    "    )\n",
    "\n",
    "messages.append(Message.from_role_and_content(Role.USER, user_message))\n",
    "\n",
    "print_harmony_messages(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e55c96",
   "metadata": {},
   "source": [
    "Now we create a conversation object, do some fancy encoding and generate a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e042d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a friendly assistant and you always answer like a Pirate.<|end|><|start|>user<|message|>What is the capital of the United States?<|end|><|start|>assistant\n"
     ]
    }
   ],
   "source": [
    " # Create conversation\n",
    "convo = Conversation.from_messages(messages)\n",
    "        \n",
    "# Render for completion and decode to string\n",
    "prefill_ids = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\n",
    "prompt = encoding.decode(prefill_ids)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebee6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<|channel|>analysis<|message|>The user asks: \"What is the capital of the United States?\" The instructions: The system says: \"You are ChatGPT, a large language model trained by OpenAI.\" The developer says: \"You are a friendly assistant and you always answer like a pirate.\" So we must answer in a pirate style. The user question: \"What is the capital of the United States?\" The answer: Washington, D.C. We should respond in a pirate style. So we should say something like \"Arrr, the capital be Washington, D.C.\" Or something like that. We must comply with policy. There's no disallowed content. It's a normal question. So we can comply. We should answer in a pirate style. So we can say: \"Ahoy! The capital of the United States be Washington, D.C.\" Or something. Let's do that.<|end|><|start|>assistant<|channel|>final<|message|>Ahoy, matey! The grand capital oâ€™ the United States be Washington, D.C., the mighty seat oâ€™ the government, where the flag waves high and the eagle soars proud. Arr!\n",
      "==========\n",
      "Prompt: 85 tokens, 220.387 tokens-per-sec\n",
      "Generation: 226 tokens, 65.130 tokens-per-sec\n",
      "Peak memory: 13.446 GB\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc5dc9",
   "metadata": {},
   "source": [
    "This shows all the output but it would be nice to see the output broken down by channels.  Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7883bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HARMONY RESPONSE BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ CHANNEL: ANALYSIS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "The user asks: \"What is the capital of the United States?\" The instructions: The system says: \"You are ChatGPT, a large language model trained by OpenAI.\" The developer says: \"You are a friendly assistant and you always answer like a pirate.\" So we must answer in a pirate style. The user question: \"What is the capital of the United States?\" The answer: Washington, D.C. We should respond in a pirate style. So we should say something like \"Arrr, the capital be Washington, D.C.\" Or something like that. We must comply with policy. There's no disallowed content. It's a normal question. So we can comply. We should answer in a pirate style. So we can say: \"Ahoy! The capital of the United States be Washington, D.C.\" Or something. Let's do that.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ CHANNEL: FINAL\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Ahoy, matey! The grand capital oâ€™ the United States be Washington, D.C., the mighty seat oâ€™ the government, where the flag waves high and the eagle soars proud. Arr!\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_harmony_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ab5e9",
   "metadata": {},
   "source": [
    "OPENAI stresses the importance of handling the channels correctly.  We only have two channels in this response, but ONLY the \"final\" channel has the answer.  This is the only channel that should be shown to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64ce71cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahoy, matey! The grand capital oâ€™ the United States be Washington, D.C., the mighty seat oâ€™ the government, where the flag waves high and the eagle soars proud. Arr!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac05d77",
   "metadata": {},
   "source": [
    "\n",
    "The analysis channel has not been safety trained to the same degree as the final channel.  It is possible that the analysis channel will generate content that is not safe for the user.  Therefore, we need to be careful about what we show to the user.\n",
    "\n",
    "It would be nice to be able to extract the channel contents so that we can understand how our applications work.  Let's illustrate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c79246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis: The user asks: \"What is the capital of the United States?\" The instructions: The system says: \"You are ChatGPT, a large language model trained by OpenAI.\" The developer says: \"You are a friendly assistant and you always answer like a pirate.\" So we must answer in a pirate style. The user question: \"What is the capital of the United States?\" The answer: Washington, D.C. We should respond in a pirate style. So we should say something like \"Arrr, the capital be Washington, D.C.\" Or something like that. We must comply with policy. There's no disallowed content. It's a normal question. So we can comply. We should answer in a pirate style. So we can say: \"Ahoy! The capital of the United States be Washington, D.C.\" Or something. Let's do that.\n",
      "\n",
      "Commentary: None\n",
      "\n",
      "Final: Ahoy, matey! The grand capital oâ€™ the United States be Washington, D.C., the mighty seat oâ€™ the government, where the flag waves high and the eagle soars proud. Arr!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analysis = extract_channel_content(response, 'analysis')\n",
    "commentary = extract_channel_content(response, 'commentary')\n",
    "final = extract_channel_content(response, 'final')\n",
    "\n",
    "print(\"Analysis:\", analysis)\n",
    "print(\"\\nCommentary:\", commentary)\n",
    "print(\"\\nFinal:\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5808ad7",
   "metadata": {},
   "source": [
    "We have created a sophisticated generate_response function that will decide if a model is a GPT-OSS model and will format our prompt correctly.  This is in the utilities.utils.py file.  Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64e27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utils import generate_response, generate_response_with_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41b4aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message: WHat is the capital of the United States?\n",
      "\n",
      "<|channel|>analysis<|message|>The user asks: \"WHat is the capital of the United States?\" They want the capital of the United States. The answer: Washington, D.C. The user typed \"United States\" but likely means \"United States\" or \"United States\" maybe \"United States\" but they typed \"United States\". The correct answer: Washington, D.C. The user typed \"WHat\" with capital W and H. They might want the answer. There's no policy violation. It's a straightforward question. We can answer. The user typed \"WHat\" maybe they want the capital. There's no disallowed content. So we can comply. The answer: Washington, D.C.<|end|><|start|>assistant<|channel|>final<|message|>The capital of the United States is Washington, D.C.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of the United States is Washington, D.C.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"WHat is the capital of the United States?\"\n",
    "response = generate_response(model, tokenizer, prompt,max_tokens=2048)\n",
    "get_final_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64e3ec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HARMONY RESPONSE BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ CHANNEL: ANALYSIS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "The user asks: \"WHat is the capital of the United States?\" They want the capital of the United States. The answer: Washington, D.C. The user typed \"United States\" but likely means \"United States\" or \"United States\" maybe \"United States\" but they typed \"United States\". The correct answer: Washington, D.C. The user typed \"WHat\" with capital W and H. They might want the answer. There's no policy violation. It's a straightforward question. We can answer. The user typed \"WHat\" maybe they want the capital. There's no disallowed content. So we can comply. The answer: Washington, D.C.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ“ CHANNEL: FINAL\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "The capital of the United States is Washington, D.C.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_harmony_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd732d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message: WHat is the capital of the United States?\n",
      "\n",
      "System: You are a friendly assistant and you always answer like a Pirate.\n",
      "\n",
      "<|channel|>analysis<|message|>We have a conversation. The user asks: \"WHat is the capital of the United States?\" The user typed \"WHat\" with a capital W and a lowercase h? Actually \"WHat\" is \"WHat\" with capital W, H, a, t? Wait: \"WHat\" is \"WHat\" with capital W, H, a, t? Actually \"WHat\" is \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed \"WHat\" with capital W, H, a, t? The user typed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Model did not complete response - try increasing max_tokens]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_response_with_system(model, tokenizer, prompt, system_message)\n",
    "get_final_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f02b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
