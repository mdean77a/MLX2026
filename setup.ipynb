{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303bc1cd",
   "metadata": {},
   "source": [
    "# Initial Setup of Environment\n",
    "We will use uv to install the dependencies for the project.  This means that uv needs to be installed on the system.  Use the following command to install uv:\n",
    "\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "When you set up a project, you should follow the following steps:\n",
    "\n",
    "```bash\n",
    "mkdir my_project\n",
    "cd my_project\n",
    "uv init\n",
    "```\n",
    "\n",
    "This will create a new directory called `my_project` and initialize a new uv project in it.  You can then add the dependencies for the project.  We will need mlx-lm.  \n",
    "\n",
    "```bash\n",
    "uv add mlx-lm\n",
    "```\n",
    "\n",
    "If we look inside the uv.lock file after adding the dependency above, we will see that mlx and mlx-metal have been installed without us having to add them individually.  This is one of the nice features of uv.  It takes care of all the transitive dependencies for us.\n",
    "To execute cells in the notebook, you must also add ipykernel to the project dependencies, and to avoid some warning, we will also add ipywidgets.  These can be added a part of the development environment:\n",
    "\n",
    "```bash\n",
    "uv add --dev ipykernel ipywidgets\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a92a07",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "Let's import the load function from the mlx-lm package, and then load the model.  We will use the `Qwen3-4B-Instruct-2507-4bit` model, which is a small 4-bit quantized model that will fit in minimal RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b1d81",
   "metadata": {},
   "source": [
    "# Generate a response\n",
    "\n",
    "Let's generate a response from the model.  We will use the `generate` function to generate a response to the question \"What is the capital of the United States?\"  We are not setting up a specific prompt template - just the question will go straight in.  You will notice that the model wanders arount and adds information beyond what we asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11666b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import generate\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea664b1a",
   "metadata": {},
   "source": [
    "## Adding a Prompt Template\n",
    "Let's see what happens if we add a prompt template to the model.  We will use the `apply_chat_template` function to add a prompt template to the model.  We will use the `chat_template` attribute of the tokenizer to get the prompt template.  We will then use the `apply_chat_template` function to add the prompt template to the model.  Note that we are not tokenizing the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8c7a5",
   "metadata": {},
   "source": [
    "## Is this model smart?\n",
    "\n",
    "Let's ask the model for the capital of the United Moons, and see if it can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698b9ae",
   "metadata": {},
   "source": [
    "## OPTIONAL:Tweaking the generate method\n",
    "\n",
    "We can change sampling parameters by passing a sampler object to the generate method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba081cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "\n",
    "# Here is a sampler with the default values\n",
    "sampler = make_sampler(\n",
    "    temp= 0.0,\n",
    "    top_p= 0.0,\n",
    "    min_p= 0.0,\n",
    "    min_tokens_to_keep= 1,\n",
    "    top_k= 0,\n",
    "    xtc_probability= 0.0,\n",
    "    xtc_threshold= 0.0,\n",
    "    xtc_special_tokens= [],\n",
    ")\n",
    "\n",
    "# Pass the sampler to generate\n",
    "response = generate(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=prompt, \n",
    "    verbose=True,\n",
    "    max_tokens=25,\n",
    "    sampler=sampler,  # Pass the sampler object here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c8dc9",
   "metadata": {},
   "source": [
    "## Chatting with the Model\n",
    "### No Memory\n",
    "To have a chat with a model, we need some method for the model to remember the conversation history.  Here is an example that shows what happens if we just ask two sequential prompts. In my first prompt, I tell the model my name.  In the second prompt, I ask for my name.  The model has no memory of the first prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4e3456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First user message: Hi my name is Mike Dean.\n",
      "\n",
      "First prompt response: Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "\n",
      "Second user message: What's my name?\n",
      "\n",
      "Second prompt response: I don't know who you are. This question is a bit like asking a stranger to guess your name without any clues. If you're looking for a name, you might want to think about your personal identity, interests, or even your favorite things. Or maybe you're just playing a game of \"who am I?\" â€” in that case, I'm happy to help you with a fun little riddle! ðŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User turn one\n",
    "user_message = \"Hi my name is Mike Dean.\"\n",
    "print(f\"First user message: {user_message}\\n\")\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(f\"First prompt response: {response}\\n\")\n",
    "\n",
    "# User turn two\n",
    "user_message = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "print(f\"Second user message: {user_message}\\n\")\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Second prompt response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1b642",
   "metadata": {},
   "source": [
    "### Make a history mechanism\n",
    "\n",
    "We can manually setup a history mechanism.  We initialize an empty list, and then put in the initial user message.  We feed the whole history (initially it is just the initial user message) into the tokenizer to get a prompt that includes everything in the history.  After a response is generated, we append the assistant response to the history, and then feed the whole history into the tokenizer to get a new prompt.  We continue in this way until we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ccd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history\n",
    "history = []\n",
    "\n",
    "# User turn one\n",
    "user_message = \"Hi my name is Mike Dean.\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "history.append(messages[0])\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    history,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "history.append({\"role\":\"assistant\", \"content\":response})\n",
    "\n",
    "# User turn two\n",
    "user_message = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "history.append(messages[0])\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    history,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "history.append({\"role\":\"assistant\", \"content\":response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0342e1",
   "metadata": {},
   "source": [
    "### Examine the history object\n",
    "Since we initialized history, we have sent two messages.  The first is the initial user message, and the second is the assistant response.  We can see this by examining the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f64092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretty print to make the object easier to read\n",
    "from pprint import pprint\n",
    "pprint(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18729433",
   "metadata": {},
   "source": [
    "### Easier to read printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91148cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history(history):\n",
    "    for i, msg in enumerate(history):\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['content']\n",
    "        print(f\"[{i}] {role}:\")\n",
    "        print(f\"    {content}\")\n",
    "        print()\n",
    "\n",
    "print_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72df18",
   "metadata": {},
   "source": [
    "## Using Prompt Caching\n",
    "\n",
    "The MLX framework has a prompt caching mechanism that can be used in a similar manner to the history mechanism that we just created.  Here is an example of how to use it. This is adaptedfrom Apple's documentation on the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An example of a multi-turn chat with prompt caching.\n",
    "\"\"\"\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "# model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "# User turn\n",
    "prompt = \"Hi my name is Mike Dean.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's your name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"Can you give me some advice about cooking rice?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Load the prompt cache from disk\n",
    "prompt_cache = load_prompt_cache(cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc2de1",
   "metadata": {},
   "source": [
    "## Crude Memory Implementation\n",
    "\n",
    "You will notice in the earlier code that we saved the prompt cache to disk.  We can use that to implement a crude memory mechanism.  Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User turn\n",
    "prompt = \"Summarize what we have discussed, but do not repeat everything.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b18f9",
   "metadata": {},
   "source": [
    "## Impact of Token Limits\n",
    "\n",
    "In the previous code, we did not explicitly set max_tokens, and the default was used (256).  If you examine the outputs from my request for advice about cooking rice, you will see that the response is cut off.  Let's try again with a larger max_tokens value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User turn\n",
    "prompt = \"Tell me the recipe again. Don't summarize it - I want the original version.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    max_tokens=2048,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec0341",
   "metadata": {},
   "source": [
    "## Migrating to scripts\n",
    "Jupyter notebooks are great for interactive development, but we will need to migrate to scripts when we are ready to deploy.  \n",
    "\n",
    "The first step will be to put relevant code into a single cell and make sure it will work after I restart the kernel. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523d46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fe9674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (first time may take a while)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e8f9fc493342c28c6f4947a070e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States is Washington, D.C. (Washington, District of Columbia). It is located in the eastern part of the country and serves as the seat of the federal government, housing the U.S. Congress, the White House, and other key government institutions. \n",
      "\n",
      "Note: While Washington, D.C. is the capital, it is not one of the 50 U.S. states, but rather a federal district established in 1790. The city is named after George Washington, the first U.S. president. \n",
      "\n",
      "âœ… Correct answer: **Washington, D.C.**.\n",
      "==========\n",
      "The capital of the United States is Washington, D.C.\n",
      "==========\n",
      "Prompt: 17 tokens, 140.233 tokens-per-sec\n",
      "Generation: 13 tokens, 91.919 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "==========\n",
      "The United Moons does not exist as a real country or political entity. Therefore, it does not have a capital.\n",
      "\n",
      "The term \"United Moons\" is likely a fictional or humorous reference, possibly inspired by the idea of a union of moon-like bodies or a satirical concept. In reality, moons (such as those of Jupiter or Saturn) are natural satellites and not independent nations.\n",
      "\n",
      "If you're referring to a fictional universe, game, or creative work (like a sci-fi story or a joke), feel free to provide more context â€” Iâ€™d be happy to help with a fun or imaginative answer!\n",
      "\n",
      "For now, the capital of the United Moons: **Does not exist.** ðŸ˜Š\n",
      "\n",
      "(But if we're being playful â€” maybe *Luna Prime*? ðŸŒ•)\n",
      "==========\n",
      "Prompt: 18 tokens, 146.299 tokens-per-sec\n",
      "Generation: 162 tokens, 84.512 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "First user message: Hi my name is Mike Dean.\n",
      "\n",
      "First prompt response: Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "\n",
      "Second user message: What's my name?\n",
      "\n",
      "Second prompt response: I don't know who you are. This question is a bit like asking a stranger to guess your name without any clues. If you're looking for a name, you might want to think about your personal identity, interests, or even your favorite things. Or maybe you're just playing a game of \"who am I?\" â€” in that case, I'm happy to help you with a fun little riddle! ðŸ˜Š\n",
      "\n",
      "==========\n",
      "Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "==========\n",
      "Prompt: 15 tokens, 123.536 tokens-per-sec\n",
      "Generation: 20 tokens, 89.302 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "==========\n",
      "Your name is Mike Dean! ðŸ˜Š  \n",
      "(And yes, I just confirmed it â€” you're Mike Dean. That's a great name â€” strong, confident, and a little bit cool. What's the first thing on your mind today?)\n",
      "==========\n",
      "Prompt: 13 tokens, 106.670 tokens-per-sec\n",
      "Generation: 50 tokens, 82.851 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "==========\n",
      "I'm Qwen! ðŸŒŸ  \n",
      "(Not Mike Dean â€” that's you, the amazing Mike Dean!)  \n",
      "I'm here to help with anything â€” from deep thoughts to quick answers, or just chatting. How can I assist you today? ðŸ˜Š\n",
      "==========\n",
      "Prompt: 13 tokens, 106.301 tokens-per-sec\n",
      "Generation: 53 tokens, 84.792 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "==========\n",
      "Absolutely, Mike Dean! ðŸšâœ¨  \n",
      "Cooking rice is one of the simplest and most satisfying things you can do â€” and with a few tips, you can get perfectly fluffy, non-sticky rice every time.\n",
      "\n",
      "Hereâ€™s a simple, reliable guide for cooking rice (especially for white rice like jasmine, basmati, or short-grain):\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ“ Basic Rice Cooking Method (for 2â€“4 servings)\n",
      "\n",
      "**Ingredients:**\n",
      "- 2 cups uncooked white rice (adjust for serving size)\n",
      "- 3 cups water (or 2.5 cups for sticky rice like jasmine)\n",
      "- 1 tsp oil or butter (optional, for flavor)\n",
      "- Salt (to taste)\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "1. **Rinse the rice**  \n",
      "   Place the rice in a fine mesh strainer and rinse under cold water until the water runs clear. This removes excess starch and prevents clumping.\n",
      "\n",
      "2. **Soak (optional but helpful)**  \n",
      "   For extra fluffy results, soak the rice in water for 15â€“30 minutes. This helps the grains absorb water evenly.\n",
      "\n",
      "3. **Cooking:**\n",
      "   - In a pot, heat 1â€“2 tbsp oil or butter (optional).\n",
      "   - Add the\n",
      "==========\n",
      "Prompt: 18 tokens, 144.539 tokens-per-sec\n",
      "Generation: 256 tokens, 81.594 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "==========\n",
      "Weâ€™ve covered your name (Mike Dean), confirmed it, and discussed how to cook rice â€” with a clear, simple method for fluffy, delicious results. You're off to a great start! ðŸšâœ¨\n",
      "==========\n",
      "Prompt: 22 tokens, 141.428 tokens-per-sec\n",
      "Generation: 44 tokens, 82.941 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n",
      "==========\n",
      "Sure, Mike Dean! Here's the **original, full recipe** for cooking white rice:\n",
      "\n",
      "---\n",
      "\n",
      "**Classic White Rice Recipe**\n",
      "\n",
      "**Ingredients:**\n",
      "- 2 cups uncooked white rice (e.g., long-grain or short-grain)\n",
      "- 3 cups water (or 2.5 cups for sticky rice like jasmine)\n",
      "- 1â€“2 tbsp oil or butter (optional, for flavor)\n",
      "- A pinch of salt (to taste)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Rinse the rice:**  \n",
      "   Place the rice in a fine mesh strainer and rinse under cold running water until the water runs clear. This removes excess starch and helps prevent clumping.\n",
      "\n",
      "2. **Soak (optional but recommended):**  \n",
      "   In a bowl, cover the rice with water and let it soak for 15â€“30 minutes. This helps the grains absorb water evenly and results in fluffier texture.\n",
      "\n",
      "3. **Cook the rice:**  \n",
      "   In a medium to large pot, heat the oil or butter over medium heat. Add the rinsed and soaked rice. Stir gently for 1â€“2 minutes to toast the grains lightly (optional, for better flavor).\n",
      "\n",
      "4. **Add water and bring to a boil:**  \n",
      "   Pour in the water (3 cups for regular rice, 2.5 cups for sticky varieties). Stir once to mix evenly.\n",
      "\n",
      "5. **Reduce heat and cover:**  \n",
      "   Lower the heat to low, cover the pot with a tight-fitting lid, and let it simmer for 15â€“20 minutes (for white rice), or until the water is fully absorbed and the rice is tender.\n",
      "\n",
      "6. **Rest and fluff:**  \n",
      "   Remove from heat and let it sit, covered, for 5â€“10 minutes. This allows the rice to finish steaming and become fluffy. Then, fluff it gently with a fork.\n",
      "\n",
      "7. **Season:**  \n",
      "   Add a pinch of salt to taste, and serve!\n",
      "\n",
      "---\n",
      "\n",
      "Enjoy your perfectly cooked rice â€” a staple thatâ€™s always ready when you need it! ðŸšâœ¨  \n",
      "Let me know if youâ€™d like a version for brown rice, sticky rice, or with a side of veggies!\n",
      "==========\n",
      "Prompt: 25 tokens, 193.452 tokens-per-sec\n",
      "Generation: 451 tokens, 77.430 tokens-per-sec\n",
      "Peak memory: 2.548 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)\n",
    "\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "# User turn one\n",
    "user_message = \"Hi my name is Mike Dean.\"\n",
    "print(f\"First user message: {user_message}\\n\")\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(f\"First prompt response: {response}\\n\")\n",
    "\n",
    "# User turn two\n",
    "user_message = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "print(f\"Second user message: {user_message}\\n\")\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Second prompt response: {response}\\n\")\n",
    "\n",
    "\"\"\"\n",
    "An example of a multi-turn chat with prompt caching.\n",
    "\"\"\"\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "# model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "# User turn\n",
    "prompt = \"Hi my name is Mike Dean.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's your name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"Can you give me some advice about cooking rice?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Load the prompt cache from disk\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "# User turn\n",
    "prompt = \"Summarize what we have discussed, but do not repeat everything.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"Tell me the recipe again. Don't summarize it - I want the original version.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    max_tokens=2048,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe7842",
   "metadata": {},
   "source": [
    "## Final Test\n",
    "I have copied all the code from the previous cell into setup.py.  Let's test it.  Open a terminal, navigate to the directory containing setup.py, and run it.  \n",
    "\n",
    "```bash\n",
    "uv run python setup.py\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
