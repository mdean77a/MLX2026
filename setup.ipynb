{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303bc1cd",
   "metadata": {},
   "source": [
    "# Initial Setup of Environment\n",
    "We will use uv to install the dependencies for the project.  This means that uv needs to be installed on the system.  Use the following command to install uv:\n",
    "\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "When you set up a project, you should follow the following steps:\n",
    "\n",
    "```bash\n",
    "mkdir my_project\n",
    "cd my_project\n",
    "uv init\n",
    "```\n",
    "\n",
    "This will create a new directory called `my_project` and initialize a new uv project in it.  You can then add the dependencies for the project.  We will need mlx-lm.  \n",
    "\n",
    "```bash\n",
    "uv add mlx-lm\n",
    "```\n",
    "\n",
    "If we look inside the uv.lock file after adding the dependency above, we will see that mlx and mlx-metal have been installed without us having to add them individually.  This is one of the nice features of uv.  It takes care of all the transitive dependencies for us.\n",
    "To execute cells in the notebook, you must also add ipykernel to the project dependencies, and to avoid some warning, we will also add ipywidgets.  These can be added a part of the development environment:\n",
    "\n",
    "```bash\n",
    "uv add --dev ipykernel ipywidgets\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a92a07",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "Let's import the load function from the mlx-lm package, and then load the model.  We will use the `Qwen3-4B-Instruct-2507-4bit` model, which is a 4-bit quantized model that will fit in 32GB of RAM. This is a \"normal\" model;  later we will try to use `gpt-oss-20b-MXFP4-Q4` which uses a completely different prompting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b698511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (first time may take a while)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dd4ed235b64fe0bd0594ababb51a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlx_lm import load\n",
    "# MODEL_ID = \"mlx-community/gpt-oss-20b-MXFP4-Q4\"  # 4-bit quantization for 32GB RAM\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b1d81",
   "metadata": {},
   "source": [
    "# Chat with the Model\n",
    "\n",
    "Let's chat with the model.  We will use the `generate` function to generate a response to the question \"What is the capital of the United States?\"  We are not setting up a specific prompt template - just the question will go straight in.  You will notice that the model wanders arount and adds information beyond what we asked.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11666b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States is Washington, D.C. (Washington, District of Columbia). It is located in the eastern part of the country and serves as the seat of the federal government, housing the U.S. Congress, the White House, and other key government institutions. \n",
      "\n",
      "Note: While Washington, D.C. is the capital, it is not one of the 50 U.S. states, but rather a federal district established in 1790. The city is named after George Washington, the first U.S. president. \n",
      "\n",
      "âœ… Correct answer: **Washington, D.C.**.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import generate\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea664b1a",
   "metadata": {},
   "source": [
    "## Adding a Prompt Template\n",
    "Let's see what happens if we add a prompt template to the model.  We will use the `apply_chat_template` function to add a prompt template to the model.  We will use the `chat_template` attribute of the tokenizer to get the prompt template.  We will then use the `apply_chat_template` function to add the prompt template to the model.  Note that we are not tokenizing the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9359548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of the United States?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "==========\n",
      "The capital of the United States is Washington, D.C.\n",
      "==========\n",
      "Prompt: 17 tokens, 136.393 tokens-per-sec\n",
      "Generation: 13 tokens, 92.263 tokens-per-sec\n",
      "Peak memory: 2.338 GB\n",
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "print(prompt)\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09bdeb",
   "metadata": {},
   "source": [
    "Let's get rid of the print statements to see what would be normal behavior.\n",
    "\n",
    "We will leave the verbose output on, which will show us the final response as well as some performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d37328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The capital of the United States is Washington, D.C.\n",
      "==========\n",
      "Prompt: 17 tokens, 137.108 tokens-per-sec\n",
      "Generation: 13 tokens, 92.284 tokens-per-sec\n",
      "Peak memory: 2.338 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8c7a5",
   "metadata": {},
   "source": [
    "## Is this model smart?\n",
    "\n",
    "Let's ask the model for the capital of the United Moons, and see if it can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f497c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The United Moons does not exist as a real country or political entity. Therefore, it does not have a capital.\n",
      "\n",
      "The term \"United Moons\" is likely a fictional or humorous reference, possibly inspired by the idea of a union of moon-like bodies or a satirical concept. In reality, moons (such as those of Jupiter or Saturn) are natural satellites and not independent nations.\n",
      "\n",
      "If you're referring to a fictional universe, game, or creative work (like a sci-fi story or a joke), feel free to provide more context â€” Iâ€™d be happy to help with a fun or imaginative answer!\n",
      "\n",
      "For now, the capital of the United Moons: **Does not exist.** ðŸ˜Š\n",
      "\n",
      "(But if we're being playful â€” maybe *Luna Prime*? ðŸŒ•)\n",
      "==========\n",
      "Prompt: 18 tokens, 142.693 tokens-per-sec\n",
      "Generation: 162 tokens, 84.931 tokens-per-sec\n",
      "Peak memory: 2.344 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698b9ae",
   "metadata": {},
   "source": [
    "## Changing the default values of the generate method\n",
    "\n",
    "We can change sampling parameters by passing a sampler object to the generate method. This is a bit of a pain, so we will use the `make_sampler` function to create a sampler object with the default values (copied from the source code in the mlx-lm package).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba081cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The United Moons does not exist as a real country or political entity. Therefore, it does not have a capital.\n",
      "\n",
      "The term \"United Moons\" is likely a fictional or humorous reference, possibly inspired by the idea of a union of moon-like bodies or a satirical concept. In reality, moons (such as those of Jupiter or Saturn) are natural satellites and not independent nations.\n",
      "\n",
      "If you're referring to a fictional universe, game, or creative work (like a sci-fi story or a joke), feel free to provide more context â€” Iâ€™d be happy to help with a fun or imaginative answer!\n",
      "\n",
      "For now, the capital of the United Moons: **Does not exist.** ðŸ˜Š\n",
      "\n",
      "(But if we're being playful â€” maybe *Luna Prime*? ðŸŒ•)\n",
      "==========\n",
      "Prompt: 18 tokens, 82.939 tokens-per-sec\n",
      "Generation: 162 tokens, 82.476 tokens-per-sec\n",
      "Peak memory: 2.344 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "\n",
    "# Here is a sampler with the default values\n",
    "sampler = make_sampler(\n",
    "    temp= 0.0,\n",
    "    top_p= 0.0,\n",
    "    min_p= 0.0,\n",
    "    min_tokens_to_keep= 1,\n",
    "    top_k= 0,\n",
    "    xtc_probability= 0.0,\n",
    "    xtc_threshold= 0.0,\n",
    "    xtc_special_tokens= [],\n",
    ")\n",
    "\n",
    "# Pass the sampler to generate\n",
    "response = generate(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=prompt, \n",
    "    verbose=True,\n",
    "    max_tokens=256,\n",
    "    sampler=sampler,  # Pass the sampler object here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec0341",
   "metadata": {},
   "source": [
    "## Migrating to scripts\n",
    "Jypyter notebooks are great for interactive development, but we will need to migrate to scripts when we are ready to deploy.  \n",
    "\n",
    "We will illustrate\n",
    "this by reproducing earlier code importing from scripts that I have saved, and then we will create an app.py file that will run everything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cfa2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (first time may take a while)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09bb4c2eff0423ba631dd714c8e24a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The United Moons does not exist as a real country or political entity. Therefore, it does not have a capital.\n",
      "\n",
      "The term \"United Moons\" is likely a fictional or humorous reference, possibly inspired by the idea of a union of moon-like bodies or a satirical concept. In reality, moons (such as those of Jupiter or Saturn) are natural satellites and not independent nations.\n",
      "\n",
      "If you're referring to a fictional universe, game, or creative work (like a sci-fi story or a joke), feel free to provide more context â€” Iâ€™d be happy to help with a fun or imaginative answer!\n",
      "\n",
      "For now, the capital of the United Moons: **Does not exist.** ðŸ˜Š\n",
      "\n",
      "(But if we're being playful â€” maybe *Luna Prime*? ðŸŒ•)\n",
      "==========\n",
      "Prompt: 18 tokens, 12.093 tokens-per-sec\n",
      "Generation: 162 tokens, 84.401 tokens-per-sec\n",
      "Peak memory: 6.874 GB\n"
     ]
    }
   ],
   "source": [
    "from setup_scripts import get_model, get_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
