{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad903da",
   "metadata": {},
   "source": [
    "\n",
    "We will start with code that we developed in the setup.ipynb notebook and refactor it into a more modular structure. Let's take a look at the code that we will be refactoring.\n",
    "\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad839b4",
   "metadata": {},
   "source": [
    "``` python\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)\n",
    "\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "```\n",
    "\n",
    "### Repetitive Code\n",
    "\n",
    "In the above code, we have a lot of repetitive code that we will want to refactor into functions.  Let's start by creating a function that will generate a response to a user message. We will include some print statements that will help us understand the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1782ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer,user_message, prompt_cache=None, **kwargs):\n",
    "    \n",
    "    if tokenizer.chat_template is not None:\n",
    "        messages = [{\"role\":\"user\", \"content\":user_message}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        prompt = user_message\n",
    "    print(f\"User message: {user_message}\\n\")\n",
    "    print(f\"{generate(model, tokenizer, prompt=prompt, verbose=True, prompt_cache=prompt_cache)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797731a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45526",
   "metadata": {},
   "source": [
    "## Adding prompt caching\n",
    "We wrote the generate_response function to take an optional prompt_cache argument.  Let's use that to add prompt caching to our code.  All have to do is pass the prompt_cache object to the generate_response function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b75289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be0816",
   "metadata": {},
   "source": [
    "Now let's save the prompt cache, and then reset it and make sure it's empty.  The model should not know my name.  Then I load the saved prompt cache and ask the same question.  The model should now know my name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf117b52",
   "metadata": {},
   "source": [
    "## Migration Cell containing the refactored code in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4156ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "def generate_response(model, tokenizer,user_message, prompt_cache=None, **kwargs):\n",
    "    \n",
    "    if tokenizer.chat_template is not None:\n",
    "        messages = [{\"role\":\"user\", \"content\":user_message}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        prompt = user_message\n",
    "    print(f\"User message: {user_message}\\n\")\n",
    "    print(f\"{generate(model, tokenizer, prompt=prompt, verbose=True, prompt_cache=prompt_cache)}\\n\")\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "# Create prompt_cache\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e239d4",
   "metadata": {},
   "source": [
    "# Modular Design\n",
    "In the code above, we defined the generate_response function in the same cell as the other code.  Let's move it to a separate file so that we can use it in other notebooks, or more importantly, in a web application.\n",
    "\n",
    "We will create a utils.py file inside of a utilities subfolder, so that we can later add other utility functions to it.  That subfolder must have a __init__.py file to make it a Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, save_prompt_cache\n",
    "from utilities import get_model, create_cache, generate_response, generate_response_with_system, list_available_models\n",
    "\n",
    "model, tokenizer, MODEL_ID = get_model(\"qwen\")\n",
    "\n",
    "list_available_models()\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\", MODEL_ID)\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "prompt_cache, cache_file = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache, _ = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9bec",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We have refactored a file with 194 lines of code (setup.py) to a file with 33 lines of code, and this file then calls the refactored code in the utilities folder.  This is a more modular design that allows us to reuse the code in other projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
