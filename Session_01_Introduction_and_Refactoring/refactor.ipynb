{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad903da",
   "metadata": {},
   "source": [
    "\n",
    "We will start with code that we developed in the setup.ipynb notebook and refactor it into a more modular structure. Let's take a look at the code that we will be refactoring.\n",
    "\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad839b4",
   "metadata": {},
   "source": [
    "``` python\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)\n",
    "\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "```\n",
    "\n",
    "### Repetitive Code\n",
    "\n",
    "In the above code, we have a lot of repetitive code that we will want to refactor into functions.  Let's start by creating a function that will generate a response to a user message. We will include some print statements that will help us understand the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1782ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer,user_message, prompt_cache=None, **kwargs):\n",
    "    \n",
    "    if tokenizer.chat_template is not None:\n",
    "        messages = [{\"role\":\"user\", \"content\":user_message}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        prompt = user_message\n",
    "    print(f\"User message: {user_message}\\n\")\n",
    "    print(f\"{generate(model, tokenizer, prompt=prompt, verbose=True, prompt_cache=prompt_cache)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797731a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45526",
   "metadata": {},
   "source": [
    "## Adding prompt caching\n",
    "We wrote the generate_response function to take an optional prompt_cache argument.  Let's use that to add prompt caching to our code.  All have to do is pass the prompt_cache object to the generate_response function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b75289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be0816",
   "metadata": {},
   "source": [
    "Now let's save the prompt cache, and then reset it and make sure it's empty.  The model should not know my name.  Then I load the saved prompt cache and ask the same question.  The model should now know my name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf117b52",
   "metadata": {},
   "source": [
    "## Migration Cell containing the refactored code in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4156ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "def generate_response(model, tokenizer,user_message, prompt_cache=None, **kwargs):\n",
    "    \n",
    "    if tokenizer.chat_template is not None:\n",
    "        messages = [{\"role\":\"user\", \"content\":user_message}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        prompt = user_message\n",
    "    print(f\"User message: {user_message}\\n\")\n",
    "    print(f\"{generate(model, tokenizer, prompt=prompt, verbose=True, prompt_cache=prompt_cache)}\\n\")\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "# Create prompt_cache\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e239d4",
   "metadata": {},
   "source": [
    "# Modular Design\n",
    "In the code above, we defined the generate_response function in the same cell as the other code.  Let's move it to a separate file so that we can use it in other notebooks, or more importantly, in a web application.\n",
    "\n",
    "We will create a utils.py file inside of a utilities subfolder, so that we can later add other utility functions to it.  That subfolder must have a __init__.py file to make it a Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "from utilities import generate_response\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "# Create prompt_cache\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load\n",
    "from mlx_lm.models.cache import load_prompt_cache, save_prompt_cache\n",
    "from utilities import generate_response, create_cache\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "prompt_cache, cache_file = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache, _ = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edeb1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mlx-community/gpt-oss-20b-MXFP4-Q4\n",
      "ðŸ”‘ Using authentication token\n",
      "(First time may take a while...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442c92ba6b342e794d9469b8a8fcebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully!\n",
      "Available Models:\n",
      "------------------------------------------------------------\n",
      "\n",
      "QWEN3-4B:\n",
      "  Full ID: mlx-community/Qwen3-4B-Instruct-2507-4bit\n",
      "  Aliases: qwen3-4b, qwen\n",
      "\n",
      "GPT-120B:\n",
      "  Full ID: mlx-community/gpt-oss-120b-MXFP4-Q4\n",
      "  Aliases: gpt-120b\n",
      "\n",
      "GPT-20B:\n",
      "  Full ID: mlx-community/gpt-oss-20b-MXFP4-Q4\n",
      "  Aliases: gpt-20b, gpt\n",
      "\n",
      "LLAMA-8B:\n",
      "  Full ID: mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\n",
      "  Aliases: llama-8b, llama\n",
      "\n",
      "MISTRAL-24B:\n",
      "  Full ID: lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-MLX-4bit\n",
      "  Aliases: mistral\n",
      "User message: What is the capital of the United States?\n",
      "\n",
      "<|channel|>analysis<|message|>The user asks: \"What is the capital of the United States?\" They likely mean \"United States\" but likely \"United States\" is a typo for \"United States\" maybe \"United States\" meaning \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United States\" maybe \"United\n",
      "\n",
      "User message: What is the capital of the United Moons?\n",
      "\n",
      "<|channel|>analysis<|message|>We have a user query: \"What is the capital of the United Moons?\" This seems like a trick question. \"United Moons\" might be a fictional country. It might be a pun: \"United Moons\" could be \"United Nations\" but with \"Moons\" maybe \"Moons\" is a pun on \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"Moons\" is \"Moons\" like \"Moons\" maybe \"\n",
      "\n",
      "User message: Hi my name is Mike Dean.\n",
      "\n",
      "<|channel|>analysis<|message|>We need to respond. The user says \"Hi my name is Mike Dean.\" We should respond politely. Probably ask how can help.<|end|><|start|>assistant<|channel|>final<|message|>Hello Mike! Nice to meet you. How can I help you today?\n",
      "\n",
      "User message: What is my name?\n",
      "\n",
      "<|channel|>analysis<|message|>We have a conversation. The user says: \"What is my name?\" They want the assistant to guess or know the user's name. The user hasn't provided their name. The user might be asking the assistant to guess or ask. The user might be expecting the assistant to respond with \"I don't know your name\" or \"I don't know your name.\" The user might be expecting the assistant to guess or ask. The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to ask for the name. The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be expecting the assistant to respond with \"I don't know your name.\" The user might be\n",
      "\n",
      "cache_files/gpt-oss-20b-MXFP4-Q4.safetensors\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m prompt_cache, cache_file = create_cache(model, MODEL_ID)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(cache_file)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi my name is Mike Dean.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m generate_response(model,tokenizer, \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms my name?\u001b[39m\u001b[33m\"\u001b[39m, prompt_cache)\n\u001b[32m     17\u001b[39m generate_response(model,tokenizer, \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms your name?\u001b[39m\u001b[33m\"\u001b[39m, prompt_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/MLX2026/utilities/utils.py:27\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(model, tokenizer, user_message, model_id, prompt_cache, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mGenerate a response to a user message using the MLX language model.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03mAutomatically handles different prompt formats based on model type.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m              (e.g., max_tokens, temperature, top_p, etc.)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Detect if this is a GPT-OSS model that requires Harmony format\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m is_gpt_oss = model_id \u001b[38;5;129;01mand\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mgpt-oss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_id\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33moss-gpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_id.lower())\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_gpt_oss:\n\u001b[32m     30\u001b[39m     prompt = _format_harmony_prompt(user_message)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, save_prompt_cache\n",
    "from utilities import get_model, create_cache, generate_response, generate_response_with_system, list_available_models\n",
    "\n",
    "model, tokenizer, MODEL_ID = get_model(\"gpt\")\n",
    "\n",
    "list_available_models()\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\", MODEL_ID)\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "prompt_cache, cache_file = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache, _ = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9bec",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We have refactored a file with 194 lines of code (setup.py) to a file with 33 lines of code, and this file then calls the refactored code in the utilities folder.  This is a more modular design that allows us to reuse the code in other projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
