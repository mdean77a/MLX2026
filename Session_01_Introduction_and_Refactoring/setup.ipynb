{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303bc1cd",
   "metadata": {},
   "source": [
    "# Initial Setup of Environment\n",
    "We will use uv to install the dependencies for the project.  This means that uv needs to be installed on the system.  Use the following command to install uv:\n",
    "\n",
    "```bash\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "When you set up a project, you should follow the following steps:\n",
    "\n",
    "```bash\n",
    "mkdir my_project\n",
    "cd my_project\n",
    "uv init\n",
    "```\n",
    "\n",
    "This will create a new directory called `my_project` and initialize a new uv project in it.  You can then add the dependencies for the project.  We will need mlx-lm.  \n",
    "\n",
    "```bash\n",
    "uv add mlx-lm\n",
    "```\n",
    "\n",
    "If we look inside the uv.lock file after adding the dependency above, we will see that mlx and mlx-metal have been installed without us having to add them individually.  This is one of the nice features of uv.  It takes care of all the transitive dependencies for us.\n",
    "To execute cells in the notebook, you must also add ipykernel to the project dependencies, and to avoid some warning, we will also add ipywidgets.  These can be added a part of the development environment:\n",
    "\n",
    "```bash\n",
    "uv add --dev ipykernel ipywidgets\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a92a07",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "Let's import the load function from the mlx-lm package, and then load the model.  We will use the `Qwen3-4B-Instruct-2507-4bit` model, which is a small 4-bit quantized model that will fit in minimal RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b1d81",
   "metadata": {},
   "source": [
    "# Generate a response\n",
    "\n",
    "Let's generate a response from the model.  We will use the `generate` function to generate a response to the question \"What is the capital of the United States?\"  We are not setting up a specific prompt template - just the question will go straight in.  You will notice that the model wanders arount and adds information beyond what we asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11666b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import generate\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea664b1a",
   "metadata": {},
   "source": [
    "## Adding a Prompt Template\n",
    "Let's see what happens if we add a prompt template to the model.  We will use the `apply_chat_template` function to add a prompt template to the model.  We will use the `chat_template` attribute of the tokenizer to get the prompt template.  We will then use the `apply_chat_template` function to add the prompt template to the model.  Note that we are not tokenizing the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8c7a5",
   "metadata": {},
   "source": [
    "## Is this model smart?\n",
    "\n",
    "Let's ask the model for the capital of the United Moons, and see if it can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698b9ae",
   "metadata": {},
   "source": [
    "## OPTIONAL:Tweaking the generate method\n",
    "\n",
    "We can change sampling parameters by passing a sampler object to the generate method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba081cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.sample_utils import make_sampler\n",
    "\n",
    "# Here is a sampler with the default values\n",
    "sampler = make_sampler(\n",
    "    temp= 0.0,\n",
    "    top_p= 0.0,\n",
    "    min_p= 0.0,\n",
    "    min_tokens_to_keep= 1,\n",
    "    top_k= 0,\n",
    "    xtc_probability= 0.0,\n",
    "    xtc_threshold= 0.0,\n",
    "    xtc_special_tokens= [],\n",
    ")\n",
    "\n",
    "# Pass the sampler to generate\n",
    "response = generate(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt=prompt, \n",
    "    verbose=True,\n",
    "    max_tokens=25,\n",
    "    sampler=sampler,  # Pass the sampler object here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c8dc9",
   "metadata": {},
   "source": [
    "## Chatting with the Model\n",
    "### No Memory\n",
    "To have a chat with a model, we need some method for the model to remember the conversation history.  Here is an example that shows what happens if we just ask two sequential prompts. In my first prompt, I tell the model my name.  In the second prompt, I ask for my name.  The model has no memory of the first prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User turn one\n",
    "user_message = \"Hi my name is Mike Dean.\"\n",
    "print(f\"First user message: {user_message}\\n\")\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(f\"First prompt response: {response}\\n\")\n",
    "\n",
    "# User turn two\n",
    "user_message = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "print(f\"Second user message: {user_message}\\n\")\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Second prompt response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1b642",
   "metadata": {},
   "source": [
    "### Make a history mechanism\n",
    "\n",
    "We can manually setup a history mechanism.  We initialize an empty list, and then put in the initial user message.  We feed the whole history (initially it is just the initial user message) into the tokenizer to get a prompt that includes everything in the history.  After a response is generated, we append the assistant response to the history, and then feed the whole history into the tokenizer to get a new prompt.  We continue in this way until we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ccd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history\n",
    "history = []\n",
    "\n",
    "# User turn one\n",
    "user_message = \"Hi my name is Mike Dean.\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "history.append(messages[0])\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    history,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "history.append({\"role\":\"assistant\", \"content\":response})\n",
    "\n",
    "# User turn two\n",
    "user_message = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "history.append(messages[0])\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    history,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "history.append({\"role\":\"assistant\", \"content\":response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0342e1",
   "metadata": {},
   "source": [
    "### Examine the history object\n",
    "Since we initialized history, we have sent two messages.  The first is the initial user message, and the second is the assistant response.  We can see this by examining the history object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f64092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretty print to make the object easier to read\n",
    "from pprint import pprint\n",
    "pprint(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18729433",
   "metadata": {},
   "source": [
    "### Easier to read printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91148cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_history(history):\n",
    "    for i, msg in enumerate(history):\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['content']\n",
    "        print(f\"[{i}] {role}:\")\n",
    "        print(f\"    {content}\")\n",
    "        print()\n",
    "\n",
    "print_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72df18",
   "metadata": {},
   "source": [
    "## Using Prompt Caching\n",
    "\n",
    "The MLX framework has a prompt caching mechanism that can be used in a similar manner to the history mechanism that we just created.  Here is an example of how to use it. This is adaptedfrom Apple's documentation on the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An example of a multi-turn chat with prompt caching.\n",
    "\"\"\"\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "# model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "# User turn\n",
    "prompt = \"Hi my name is Mike Dean.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's your name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"Can you give me some advice about cooking rice?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Load the prompt cache from disk\n",
    "prompt_cache = load_prompt_cache(cache_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc2de1",
   "metadata": {},
   "source": [
    "## Crude Memory Implementation\n",
    "\n",
    "You will notice in the earlier code that we saved the prompt cache to disk.  We can use that to implement a crude memory mechanism.  Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User turn\n",
    "prompt = \"Summarize what we have discussed, but do not repeat everything.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b18f9",
   "metadata": {},
   "source": [
    "## Impact of Token Limits\n",
    "\n",
    "In the previous code, we did not explicitly set max_tokens, and the default was used (256).  If you examine the outputs from my request for advice about cooking rice, you will see that the response is cut off.  Let's try again with a larger max_tokens value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User turn\n",
    "prompt = \"Tell me the recipe again. Don't summarize it - I want the original version.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    max_tokens=2048,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec0341",
   "metadata": {},
   "source": [
    "## Migrating to scripts\n",
    "Jupyter notebooks are great for interactive development, but we will need to migrate to scripts when we are ready to deploy.  \n",
    "\n",
    "The first step will be to put relevant code into a single cell and make sure it will work after I restart the kernel. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)\n",
    "\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "# User turn one\n",
    "user_message = \"Hi my name is Mike Dean.\"\n",
    "print(f\"First user message: {user_message}\\n\")\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(f\"First prompt response: {response}\\n\")\n",
    "\n",
    "# User turn two\n",
    "user_message = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "print(f\"Second user message: {user_message}\\n\")\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"Second prompt response: {response}\\n\")\n",
    "\n",
    "\"\"\"\n",
    "An example of a multi-turn chat with prompt caching.\n",
    "\"\"\"\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "# model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "# User turn\n",
    "prompt = \"Hi my name is Mike Dean.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's my name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"What's your name?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"Can you give me some advice about cooking rice?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Load the prompt cache from disk\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "# User turn\n",
    "prompt = \"Summarize what we have discussed, but do not repeat everything.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    prompt_cache=prompt_cache,\n",
    ")\n",
    "\n",
    "# User turn\n",
    "prompt = \"Tell me the recipe again. Don't summarize it - I want the original version.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Assistant response\n",
    "response = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    max_tokens=2048,\n",
    "    prompt_cache=prompt_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe7842",
   "metadata": {},
   "source": [
    "## Final Test\n",
    "I have copied all the code from the previous cell into setup.py.  Let's test it.  Open a terminal, navigate to the directory containing setup.py, and run it.  \n",
    "\n",
    "```bash\n",
    "uv run python setup.py\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
