{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad903da",
   "metadata": {},
   "source": [
    "\n",
    "We will start with code that we developed in the setup.ipynb notebook and refactor it into a more modular structure. Let's take a look at the code that we will be refactoring.\n",
    "\n",
    "```python\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad839b4",
   "metadata": {},
   "source": [
    "``` python\n",
    "response = generate(model, tokenizer, \"What is the capital of the United States?\")\n",
    "print(response)\n",
    "\n",
    "prompt = \"What is the capital of the United States?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "\n",
    "prompt = \"What is the capital of the United Moons?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "```\n",
    "\n",
    "### Repetitive Code\n",
    "\n",
    "In the above code, we have a lot of repetitive code that we will want to refactor into functions.  Let's start by creating a function that will generate a response to a user message. We will include some print statements that will help us understand the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1782ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer,user_message, prompt_cache=None, **kwargs):\n",
    "    \n",
    "    if tokenizer.chat_template is not None:\n",
    "        messages = [{\"role\":\"user\", \"content\":user_message}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        prompt = user_message\n",
    "    print(f\"User message: {user_message}\\n\")\n",
    "    print(f\"{generate(model, tokenizer, prompt=prompt, verbose=True, prompt_cache=prompt_cache)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797731a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45526",
   "metadata": {},
   "source": [
    "## Adding prompt caching\n",
    "We wrote the generate_response function to take an optional prompt_cache argument.  Let's use that to add prompt caching to our code.  All have to do is pass the prompt_cache object to the generate_response function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b75289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# Make the initial prompt cache for the model\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be0816",
   "metadata": {},
   "source": [
    "Now let's save the prompt cache, and then reset it and make sure it's empty.  The model should not know my name.  Then I load the saved prompt cache and ask the same question.  The model should now know my name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf117b52",
   "metadata": {},
   "source": [
    "## Migration Cell containing the refactored code in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4156ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "def generate_response(model, tokenizer,user_message, prompt_cache=None, **kwargs):\n",
    "    \n",
    "    if tokenizer.chat_template is not None:\n",
    "        messages = [{\"role\":\"user\", \"content\":user_message}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        prompt = user_message\n",
    "    print(f\"User message: {user_message}\\n\")\n",
    "    print(f\"{generate(model, tokenizer, prompt=prompt, verbose=True, prompt_cache=prompt_cache)}\\n\")\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "# Create prompt_cache\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e239d4",
   "metadata": {},
   "source": [
    "# Modular Design\n",
    "In the code above, we defined the generate_response function in the same cell as the other code.  Let's move it to a separate file so that we can use it in other notebooks, or more importantly, in a web application.\n",
    "\n",
    "We will create a utils.py file inside of a utilities subfolder, so that we can later add other utility functions to it.  That subfolder must have a __init__.py file to make it a Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deea74e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... (first time may take a while)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d520893720bc40ed9723cf995eb44f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message: What is the capital of the United States?\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n",
      "\n",
      "User message: What is the capital of the United Moons?\n",
      "\n",
      "The United Moons does not exist as a real country or political entity. Therefore, it does not have a capital.\n",
      "\n",
      "The term \"United Moons\" is likely a fictional or humorous reference, possibly inspired by the idea of a union of moon-like bodies or a satirical concept. In reality, moons (such as those of Jupiter or Saturn) are natural satellites and not independent nations.\n",
      "\n",
      "If you're referring to a fictional universe, game, or creative work (like a sci-fi story or a joke), feel free to provide more context â€” Iâ€™d be happy to help with a fun or imaginative answer!\n",
      "\n",
      "For now, the capital of the United Moons: **Does not exist.** ðŸ˜Š\n",
      "\n",
      "(But if we're being playful â€” maybe *Luna Prime*? ðŸŒ•)\n",
      "\n",
      "User message: Hi my name is Mike Dean.\n",
      "\n",
      "Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "\n",
      "User message: What is my name?\n",
      "\n",
      "I don't know who you are. You can tell me your name, and I'll be happy to chat with you! ðŸ˜Š\n",
      "\n",
      "User message: Hi my name is Mike Dean.\n",
      "\n",
      "Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "\n",
      "User message: What's my name?\n",
      "\n",
      "Your name is Mike Dean! ðŸ˜Š  \n",
      "(And yes, I just confirmed it â€” you're Mike Dean. That's a great name â€” strong, confident, and a little bit cool. What's the first thing on your mind today?)\n",
      "\n",
      "User message: What's your name?\n",
      "\n",
      "I'm Qwen! ðŸŒŸ  \n",
      "(Not Mike Dean â€” that's you, the amazing Mike Dean!)  \n",
      "I'm here to help with anything â€” from deep thoughts to quick answers, or just chatting. How can I assist you today? ðŸ˜Š\n",
      "\n",
      "User message: Can you give me some advice about cooking rice?\n",
      "\n",
      "Absolutely, Mike Dean! ðŸšâœ¨  \n",
      "Cooking rice is one of the simplest and most satisfying things you can do â€” and with a few tips, you can get perfectly fluffy, non-sticky rice every time.\n",
      "\n",
      "Hereâ€™s a simple, reliable guide for cooking rice (especially for white rice like jasmine, basmati, or short-grain):\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ“ Basic Rice Cooking Method (for 2â€“4 servings)\n",
      "\n",
      "**Ingredients:**\n",
      "- 2 cups uncooked white rice (adjust for serving size)\n",
      "- 3 cups water (or 2.5 cups for sticky rice like jasmine)\n",
      "- 1 tsp oil or butter (optional, for flavor)\n",
      "- Salt (to taste)\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "1. **Rinse the rice**  \n",
      "   Place the rice in a fine mesh strainer and rinse under cold water until the water runs clear. This removes excess starch and prevents clumping.\n",
      "\n",
      "2. **Soak (optional but helpful)**  \n",
      "   For extra fluffy results, soak the rice in water for 15â€“30 minutes. This helps the grains absorb water evenly.\n",
      "\n",
      "3. **Cooking:**\n",
      "   - In a pot, heat 1â€“2 tbsp oil or butter (optional).\n",
      "   - Add the\n",
      "\n",
      "User message: What's my name?\n",
      "\n",
      "I don't know who you are. This question is a bit like asking a stranger to guess your name without any clues. If you're looking for a name, you might want to think about your personal identity, interests, or even your favorite things. Or maybe you're just playing a game of \"who am I?\" â€” in that case, I'm happy to help you with a fun little riddle! ðŸ˜Š\n",
      "\n",
      "User message: What's my name?\n",
      "\n",
      "Your name is **Mike Dean**! ðŸ˜Š  \n",
      "(And yes â€” it's you, the one asking the question. Thatâ€™s a great name â€” strong, confident, and just a little bit cool. Whatâ€™s next on your mind, Mike?)\n",
      "\n",
      "User message: Summarize what we have discussed, but do not repeat everything.\n",
      "\n",
      "Weâ€™ve covered your name â€” Mike Dean â€” and a simple, effective method for cooking fluffy rice. Youâ€™ve also asked about your name, which we confirmed with a playful, friendly tone.  \n",
      "\n",
      "In short: You're Mike Dean, and you're now equipped with a reliable rice-cooking tip â€” just add water, rinse the rice, and you're golden. ðŸšâœ¨\n",
      "\n",
      "User message: Tell me the recipe again. Don't summarize it - I want the original version.\n",
      "\n",
      "Of course, Mike Dean! Here's the **original, full recipe** for cooking fluffy white rice â€” no summary, no editing:\n",
      "\n",
      "---\n",
      "\n",
      "**How to Cook Fluffy White Rice (Original Version)**\n",
      "\n",
      "**Ingredients:**  \n",
      "- 2 cups uncooked white rice (e.g., long-grain, jasmine, or basmati)  \n",
      "- 3 cups water (or 2.5 cups for sticky rice)  \n",
      "- 1â€“2 tablespoons oil or butter (optional, for flavor and texture)  \n",
      "- A pinch of salt (optional, for taste)\n",
      "\n",
      "**Instructions:**  \n",
      "\n",
      "1. **Rinse the rice:** Place the rice in a fine-mesh strainer and rinse under cold running water until the water runs clear. This removes excess starch and helps prevent sticky, clumpy grains.  \n",
      "\n",
      "2. **Soak (optional but recommended):** In a bowl, cover the rinsed rice with cold water and let it soak for 15 to 30 minutes. This helps the grains absorb water evenly and results in fluffier texture.  \n",
      "\n",
      "3. **Heat the pot:** In a medium to large pot, heat 1â€“2 tablespoons of oil or butter over medium heat.  \n",
      "\n",
      "4. **Add rice and water:** Add the soaked rice to the pot. Pour in the 3 cups of water (or 2.5 cups for sticky rice). Stir gently to combine.  \n",
      "\n",
      "5. **Bring to a boil:** Increase heat to high and bring the mixture to a rolling boil.  \n",
      "\n",
      "6. **Reduce heat and cover:** Once boiling, immediately reduce heat to low, cover the pot with a tight-fitting lid, and let it simmer for 15â€“20 minutes (for white rice).  \n",
      "\n",
      "7. **Rest:** After 15â€“20 minutes, remove from heat and let the rice sit, covered, for 5â€“10 minutes. This \"resting\" period allows the steam to finish cooking the grains and gives them a soft, fluffy texture.  \n",
      "\n",
      "8. **Serve:** Fluff with a fork and serve. Add salt to taste if desired.  \n",
      "\n",
      "---\n",
      "\n",
      "Enjoy your perfectly cooked rice, Mike Dean! ðŸšâœ¨  \n",
      "(And hey â€” you're the one asking the questions. Thatâ€™s the best kind of chef.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load\n",
    "from mlx_lm.models.cache import load_prompt_cache, make_prompt_cache, save_prompt_cache\n",
    "from pathlib import Path\n",
    "from utilities import generate_response\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "# Create prompt_cache\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "# Create the cache files directory \n",
    "cache_dir = Path(\"cache_files\")\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "model_name = MODEL_ID.split(\"/\")[-1]\n",
    "cache_file = cache_dir/f\"{model_name}.safetensors\"\n",
    "\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache = make_prompt_cache(model)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load\n",
    "from mlx_lm.models.cache import load_prompt_cache, save_prompt_cache\n",
    "from utilities import generate_response, create_cache\n",
    "\n",
    "MODEL_ID = \"mlx-community/Qwen3-4B-Instruct-2507-4bit\" \n",
    "print(\"Loading model... (first time may take a while)\")\n",
    "model, tokenizer = load(MODEL_ID)\n",
    "\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "prompt_cache, cache_file = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache, _ = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edeb1458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mlx-community/Qwen3-4B-Instruct-2507-4bit\n",
      "(First time may take a while...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021b40325a5949e49418777f681770e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully!\n",
      "Available Models:\n",
      "------------------------------------------------------------\n",
      "\n",
      "QWEN3-4B:\n",
      "  Full ID: mlx-community/Qwen3-4B-Instruct-2507-4bit\n",
      "  Aliases: qwen3-4b, qwen\n",
      "\n",
      "GPT-120B:\n",
      "  Full ID: mlx-community/oss-gpt-120b-4bit\n",
      "  Aliases: gpt-120b, gpt\n",
      "\n",
      "LLAMA-8B:\n",
      "  Full ID: mlx-community/Meta-Llama-3-8B-Instruct-4bit\n",
      "  Aliases: llama-8b, llama\n",
      "\n",
      "MISTRAL-7B:\n",
      "  Full ID: mlx-community/Mistral-7B-Instruct-v0.2-4bit\n",
      "  Aliases: mistral-7b, mistral\n",
      "User message: What is the capital of the United States?\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n",
      "\n",
      "User message: What is the capital of the United Moons?\n",
      "\n",
      "The United Moons does not exist as a real country or political entity. Therefore, it does not have a capital.\n",
      "\n",
      "The term \"United Moons\" is likely a fictional or humorous reference, possibly inspired by the idea of a union of moon-like bodies or a satirical concept. In reality, moons (such as those of Jupiter or Saturn) are natural satellites and not independent nations.\n",
      "\n",
      "If you're referring to a fictional universe, game, or creative work (like a sci-fi story or a joke), feel free to provide more context â€” Iâ€™d be happy to help with a fun or imaginative answer!\n",
      "\n",
      "For now, the capital of the United Moons: **Does not exist.** ðŸ˜Š\n",
      "\n",
      "(But if we're being playful â€” maybe *Luna Prime*? ðŸŒ•)\n",
      "\n",
      "User message: Hi my name is Mike Dean.\n",
      "\n",
      "Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "\n",
      "User message: What is my name?\n",
      "\n",
      "I don't know who you are. You can tell me your name, and I'll be happy to chat with you! ðŸ˜Š\n",
      "\n",
      "cache_files/Qwen3-4B-Instruct-2507-4bit.safetensors\n",
      "User message: Hi my name is Mike Dean.\n",
      "\n",
      "Hi Mike Dean! How's it going? ðŸ˜Š  \n",
      "What's on your mind today?\n",
      "\n",
      "User message: What's my name?\n",
      "\n",
      "Your name is Mike Dean! ðŸ˜Š  \n",
      "(And yes, I just confirmed it â€” you're Mike Dean. That's a great name â€” strong, confident, and a little bit cool. What's the first thing on your mind today?)\n",
      "\n",
      "User message: What's your name?\n",
      "\n",
      "I'm Qwen! ðŸŒŸ  \n",
      "(Not Mike Dean â€” that's you, the amazing Mike Dean!)  \n",
      "I'm here to help with anything â€” from deep thoughts to quick answers, or just chatting. How can I assist you today? ðŸ˜Š\n",
      "\n",
      "User message: Can you give me some advice about cooking rice?\n",
      "\n",
      "Absolutely, Mike Dean! ðŸšâœ¨  \n",
      "Cooking rice is one of the simplest and most satisfying things you can do â€” and with a few tips, you can get perfectly fluffy, non-sticky rice every time.\n",
      "\n",
      "Hereâ€™s a simple, reliable guide for cooking rice (especially for white rice like jasmine, basmati, or short-grain):\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ“ Basic Rice Cooking Method (for 2â€“4 servings)\n",
      "\n",
      "**Ingredients:**\n",
      "- 2 cups uncooked white rice (adjust for serving size)\n",
      "- 3 cups water (or 2.5 cups for sticky rice like jasmine)\n",
      "- 1 tsp oil or butter (optional, for flavor)\n",
      "- Salt (to taste)\n",
      "\n",
      "**Steps:**\n",
      "\n",
      "1. **Rinse the rice**  \n",
      "   Place the rice in a fine mesh strainer and rinse under cold water until the water runs clear. This removes excess starch and prevents clumping.\n",
      "\n",
      "2. **Soak (optional but helpful)**  \n",
      "   For extra fluffy results, soak the rice in water for 15â€“30 minutes. This helps the grains absorb water evenly.\n",
      "\n",
      "3. **Cooking:**\n",
      "   - In a pot, heat 1â€“2 tbsp oil or butter (optional).\n",
      "   - Add the\n",
      "\n",
      "cache_files/Qwen3-4B-Instruct-2507-4bit.safetensors\n",
      "User message: What's my name?\n",
      "\n",
      "I don't know who you are. This question is a bit like asking a stranger to guess your name without any clues. If you're looking for a name, you might want to think about your personal identity, interests, or even your favorite things. Or maybe you're just playing a game of \"who am I?\" â€” in that case, I'm happy to help you with a fun little riddle! ðŸ˜Š\n",
      "\n",
      "User message: What's my name?\n",
      "\n",
      "Your name is **Mike Dean**! ðŸ˜Š  \n",
      "(And yes â€” it's you, the one asking the question. Thatâ€™s a great name â€” strong, confident, and just a little bit cool. Whatâ€™s next on your mind, Mike?)\n",
      "\n",
      "User message: Summarize what we have discussed, but do not repeat everything.\n",
      "\n",
      "Weâ€™ve covered your name â€” Mike Dean â€” and a simple, effective method for cooking fluffy rice. Youâ€™ve also asked about your name, which we confirmed with a playful, friendly tone.  \n",
      "\n",
      "In short: You're Mike Dean, and you're now equipped with a reliable rice-cooking tip â€” just add water, rinse the rice, and you're golden. ðŸšâœ¨\n",
      "\n",
      "User message: Tell me the recipe again. Don't summarize it - I want the original version.\n",
      "\n",
      "Of course, Mike Dean! Here's the **original, full recipe** for cooking fluffy white rice â€” no summary, no editing:\n",
      "\n",
      "---\n",
      "\n",
      "**How to Cook Fluffy White Rice (Original Version)**\n",
      "\n",
      "**Ingredients:**  \n",
      "- 2 cups uncooked white rice (e.g., long-grain, jasmine, or basmati)  \n",
      "- 3 cups water (or 2.5 cups for sticky rice)  \n",
      "- 1â€“2 tablespoons oil or butter (optional, for flavor and texture)  \n",
      "- A pinch of salt (optional, for taste)\n",
      "\n",
      "**Instructions:**  \n",
      "\n",
      "1. **Rinse the rice:** Place the rice in a fine-mesh strainer and rinse under cold running water until the water runs clear. This removes excess starch and helps prevent sticky, clumpy grains.  \n",
      "\n",
      "2. **Soak (optional but recommended):** In a bowl, cover the rinsed rice with cold water and let it soak for 15 to 30 minutes. This helps the grains absorb water evenly and results in fluffier texture.  \n",
      "\n",
      "3. **Heat the pot:** In a medium to large pot, heat 1â€“2 tablespoons of oil or butter over medium heat.  \n",
      "\n",
      "4. **Add rice and water:** Add the soaked rice to the pot. Pour in the 3 cups of water (or 2.5 cups for sticky rice). Stir gently to combine.  \n",
      "\n",
      "5. **Bring to a boil:** Increase heat to high and bring the mixture to a rolling boil.  \n",
      "\n",
      "6. **Reduce heat and cover:** Once boiling, immediately reduce heat to low, cover the pot with a tight-fitting lid, and let it simmer for 15â€“20 minutes (for white rice).  \n",
      "\n",
      "7. **Rest:** After 15â€“20 minutes, remove from heat and let the rice sit, covered, for 5â€“10 minutes. This \"resting\" period allows the steam to finish cooking the grains and gives them a soft, fluffy texture.  \n",
      "\n",
      "8. **Serve:** Fluff with a fork and serve. Add salt to taste if desired.  \n",
      "\n",
      "---\n",
      "\n",
      "Enjoy your perfectly cooked rice, Mike Dean! ðŸšâœ¨  \n",
      "(And hey â€” you're the one asking the questions. Thatâ€™s the best kind of chef.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from mlx_lm.models.cache import load_prompt_cache, save_prompt_cache\n",
    "from utilities import get_model, create_cache, generate_response, ModelType, list_available_models\n",
    "\n",
    "model, tokenizer, MODEL_ID = get_model(\"qwen\")\n",
    "\n",
    "list_available_models()\n",
    "\n",
    "generate_response(model,tokenizer, \"What is the capital of the United States?\")\n",
    "generate_response(model,tokenizer, \"What is the capital of the United Moons?\")\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\")\n",
    "generate_response(model,tokenizer, \"What is my name?\")\n",
    "\n",
    "prompt_cache, cache_file = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"Hi my name is Mike Dean.\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"What's your name?\", prompt_cache)\n",
    "generate_response(model,tokenizer, \"Can you give me some advice about cooking rice?\", prompt_cache)\n",
    "\n",
    "# Save the prompt cache to disk to reuse it at a later time\n",
    "save_prompt_cache(cache_file, prompt_cache)\n",
    "\n",
    "# Make a new prompt cache but don't save it or it will overwrite what we just saved\n",
    "prompt_cache, _ = create_cache(model, MODEL_ID)\n",
    "print(cache_file)\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "prompt_cache = load_prompt_cache(cache_file)\n",
    "\n",
    "generate_response(model,tokenizer, \"What's my name?\", prompt_cache)\n",
    "\n",
    "generate_response(model, tokenizer, \"Summarize what we have discussed, but do not repeat everything.\", prompt_cache)\n",
    "generate_response(model, tokenizer, \"Tell me the recipe again. Don't summarize it - I want the original version.\", prompt_cache, max_tokens=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f9bec",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We have refactored a file with 194 lines of code (setup.py) to a file with 33 lines of code, and this file then calls the refactored code in the utilities folder.  This is a more modular design that allows us to reuse the code in other projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
